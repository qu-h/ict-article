{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "## The Perceptron:\n",
    "\n",
    "Inspired by the biological design of brain, in particular the neuron, Frank Rosenblatt invented the perceptron which had the same biological blueprint like that of a neuron.\n",
    "\n",
    "![Image](https://appliedgo.net/media/perceptron/neuron.png)\n",
    "\n",
    "In perceptron the data is collected through many inputs, all are then subjected to pre-activation step, generally addition and then are activated or the result of pre-activation is passed down to activation function which generates a result which is called activation value.\n",
    "\n",
    "Similar to what happens in a neuron, dendrites collect the single and the nucleus process the singnal and the signal is transferred further.\n",
    "\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "The perceptron classifies instances by processing a linear combination of the explanatory variables and the model parameters using an activation function. The linear combination of the parameters and inputs is called the perceptron's preactivation.\n",
    "\n",
    "![Perceptron Formula](http://blog.zabarauskas.com/img/perceptron.gif)\n",
    "\n",
    "Several different activation functions are commonly used. Rosenblatt's\n",
    "original perceptron used the Heaviside step function. Also called the unit step function.\n",
    "\n",
    "Another common activation function is the logistic sigmoid activation function. The gradients for this activation function can be calculated efficiently, which will be important property for artificial neural networks.\n",
    "\n",
    "\n",
    "## The perceptron learning alogorithm\n",
    "\n",
    "![Image](http://ama.liglab.fr/~amini/Perceptron/perceptron.png)\n",
    "\n",
    "The perceptron learning algorithm begins by setting the weights to zero or to small random values. It then predicts the class for a training instance. The perceptron is an error-driven learning algorithm; if the prediction is correct, the algorithm continues to the next instance.\n",
    "\n",
    "Scikit-learn provides an implementation of the perceptron. As with the other implementations that we used, the constructor for the Perceptron class accepts keyword arguments that set the algorithm's hyperparameters. Perceptron similarly\n",
    "exposes the fit_transform() and predict() methods. Perceptron also provides a partial_fit() method, which allows the classifier to train and make predictions for streaming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakhar/Desktop/scikit/venv/local/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True,\n",
       "      max_iter=None, n_iter=100, n_jobs=1, penalty=None, random_state=0,\n",
       "      shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "categories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\n",
    "\n",
    "newgroups_train = fetch_20newsgroups(subset = 'train',\n",
    "                    categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                    categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(newgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "classifier = Perceptron(n_iter = 100, eta0=0.1)\n",
    "classifier.fit(X_train, newgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe that Rusty Staub was also a jewish ball-player\n",
      "Also, Mordaci Brown back in the early 20th century.  He was a pitcher whose\n",
      "nickname was \"3 fingers\" Brown....for obvious reasons....he had 3 fingers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print newgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.92      0.89       396\n",
      "          1       0.85      0.81      0.83       397\n",
      "          2       0.89      0.86      0.87       399\n",
      "\n",
      "avg / total       0.86      0.86      0.86      1192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "print classification_report(newsgroups_test.target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Perceptron\n",
    "\n",
    "While the perceptron classified the instances in our example well, the model has limitations. Linear models like the perceptron with a Heaviside activation function are not universal function approximators; they cannot represent some functions. Specifically, linear models can only learn to approximate the functions for linearly separable datasets. The linear classifiers that we have examined find a hyperplane that separates the positive classes from the negative classes; if no hyperplane exists that can separate the classes, the problem is not linearly separable. A simple example of a function that is linearly inseparable is the logical operation XOR, or exclusive disjunction. The output of XOR is one when one of its inputs is equal to one and the other is equal to zero.\n",
    "\n",
    "![image](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/images/perceptron/perceptron_img4.png)\n",
    "\n",
    "It is impossible to separate the circles from the diamonds using a single straight line. Suppose that the instances are pegs on a board. If you were to stretch a rubber band around both of the positive instances, and stretch a second rubber band around both\n",
    "of the negative instances, the bands would intersect in the middle of the board. The rubber bands represent convex hulls, or the envelope that contains all of the points within the set and all of the points along any line connecting a pair points within the set. \n",
    "\n",
    "\n",
    "## From the Perceptron to Artificial Neural Networks\n",
    "\n",
    "The perceptron is not a universal function approximator; its decision boundary must be a hyperplane. **Artificial neural networks**, which are powerful nonlinear models for classification and regression that use a different strategy to overcome the perceptron's limitations.\n",
    "If the perceptron is analogous to a neuron, an artificial neural network, or neural net, is analogous to a brain. As billions of neurons with trillions of synapses comprise a human brain, an artificial neural network is a directed graph of perceptrons or other artificial neurons. The graph's edges are weighted; these weights are the parameters of the model that must be learned.\n",
    "\n",
    "## Feedforward and Feedback Artifical Neural Nets\n",
    "\n",
    "Artificial neural networks are described by three components. The first is the model's architecture, or topology, which describes the layers of neurons and structure of the connections between them. The second component is the activation function used by the artificial neurons. The third component is the learning algorithm that finds the optimal values of the weights.\n",
    "\n",
    "## Multilayer Perceptron (MLP)\n",
    "The multilayer perceptron (MLP) is the one of the most commonly used artificial neural networks. The name is a slight misnomer; a multilayer perceptron is not a single perceptron with multiple layers, but rather multiple layers of artificial neurons that can be perceptrons. The layers of the MLP form a directed, acyclic graph.\n",
    "Generally, each layer is fully connected to the subsequent layer; the output of each artificial neuron in a layer is an input to every artificial neuron in the next layer towards the output. MLPs have three or more layers of artificial neurons. The input layer consists of simple input neurons. The input neurons are connected to at least one hidden layer of artificial neurons. The hidden layer represents latent variables; the input and output of this layer cannot be observed in the training data. Finally, the last hidden layer is connected to an output layer.\n",
    "\n",
    "Theoritical explanation of feedforward and backward prop.\n",
    "\n",
    "![image](https://www.researchgate.net/profile/Abdelazim_Negm/publication/273768094/figure/fig2/AS:294800436809735@1447297309947/Figure-4-A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=2, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=3, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "y = [0, 1, 1, 0] * 100000\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]] * 100000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state =3)\n",
    "\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes = (2), \n",
    "                          activation = 'logistic',\n",
    "                          solver = 'adam',\n",
    "                          random_state =3)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     49923\n",
      "          1       1.00      1.00      1.00     50077\n",
      "\n",
      "avg / total       1.00      1.00      1.00    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94518272  0.94657763  0.89261745]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('ss', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes =150, alpha = 0.1, solver='adam' ))\n",
    "])\n",
    "\n",
    "print cross_val_score(pipeline, X, y, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=150, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = StandardScaler()\n",
    "X = scale.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes =150, alpha = 0.1, solver='adam' )\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        52\n",
      "          1       0.94      1.00      0.97        46\n",
      "          2       1.00      1.00      1.00        47\n",
      "          3       1.00      1.00      1.00        45\n",
      "          4       1.00      0.96      0.98        45\n",
      "          5       0.96      1.00      0.98        44\n",
      "          6       1.00      0.98      0.99        47\n",
      "          7       1.00      0.98      0.99        44\n",
      "          8       0.97      0.97      0.97        37\n",
      "          9       0.98      0.95      0.96        43\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important links for reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit",
   "language": "python",
   "name": "scikit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
